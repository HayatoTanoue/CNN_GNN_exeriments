{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# robustness 検証\n",
    "\n",
    "ヒートマップ作成用のデータフレームの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from reserch_utils_HT import network_to_image\n",
    "from models.set_model import CNN_base, D1D2_base\n",
    "from data.data_loader import cnn_data_loader_cv, set_transform\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from natsort import natsorted\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_to_tensor(resize, kind, path):\n",
    "    \"\"\" データの読み込み, tensor結合 \"\"\"\n",
    "    transform = set_transform(resize)\n",
    "    for i in range(100):\n",
    "        img = Image.open(f\"{path}/{i}.png\")\n",
    "        if i == 0:\n",
    "            data = transform(img).view(1,1,resize,resize)\n",
    "        else:\n",
    "            img_tensor = transform(img).view(1,1,resize,resize)\n",
    "            data = torch.cat((data, img_tensor), 0)\n",
    "    return data\n",
    "\n",
    "def make_pred_df(model, data, kind, n, p, cnt):\n",
    "    \"\"\" ネットワークごとの予測ラベルと尤度データ作成 \"\"\"\n",
    "    kind_to_label = {\"BA\": 0, \"Attach\": 1, \"Growth\": 2, \"Random\": 3}\n",
    "    softmax = torch.nn.Softmax(1)\n",
    "    with torch.no_grad():\n",
    "        pred = softmax(model(data))\n",
    "    index = pred.argmax(dim=1) # pred index\n",
    "\n",
    "    pred_df = pd.DataFrame()\n",
    "    pred_df[\"seed\"] = np.array(range(100)) + 10000 # seed\n",
    "    pred_df[\"node\"] = n\n",
    "    pred_df[\"parameter\"] = p\n",
    "    pred_df[\"kind\"] = kind\n",
    "    pred_df[\"true_label\"] = kind_to_label[kind]\n",
    "    pred_df[\"pred\"] = index\n",
    "    pred_df[\"probablility\"] = [pred[i, idx].item() for i, idx in enumerate(index)]\n",
    "    \n",
    "    return pred_df\n",
    "\n",
    "def robust_acc_df(model, resize, kind):\n",
    "    kind_to_index = {\"BA\": 0, \"Attach\": 1, \"Growth\": 2, \"Random\": 3}\n",
    "    transform = set_transform(resize)\n",
    "    df = pd.DataFrame()\n",
    "    cnt = 0\n",
    "    for n in tqdm([20,30,50,70,100,200,300,500,700,1000,2000]):\n",
    "        if kind == \"Random\":\n",
    "            paths = sorted(glob(f\"./robustness_data_img/{kind}/{n}/*\"))\n",
    "        else:\n",
    "            paths = natsorted(glob(f\"./robustness_data_img/{kind}/{n}/*\"))\n",
    "        for path_index, path in enumerate(paths):\n",
    "            # load data\n",
    "            p = float(path.split(\"/\")[-1])\n",
    "            # image to torch tensor\n",
    "            data = load_image_to_tensor(resize, kind, path)\n",
    "            pred_df = make_pred_df(model, data, kind, n, p, cnt)\n",
    "            cnt += 100\n",
    "            df = df.append(pred_df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_heatmap(dataset_name, resize, gpu=True):\n",
    "    # load model\n",
    "    model = CNN_base(\"CNN\", 4, resize)\n",
    "    model_path = f\"../logs/{dataset_name}/CNN/sort_{resize}_0.001/model_weight/fold0_trial0_epoch40.pth\"\n",
    "    if gpu:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    # make acc dataframe\n",
    "    pred_df = pd.DataFrame()\n",
    "    for kind in [\"BA\", \"Attach\", \"Growth\", \"Random\"]:\n",
    "        pred_df = pred_df.append(robust_acc_df(model, resize, kind))\n",
    "    pred_df.to_csv(f\"./robustness_plot/CNN_{dataset_name}_{resize}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:02<00:00,  5.73s/it]\n",
      "100%|██████████| 11/11 [01:13<00:00,  6.65s/it]\n",
      "100%|██████████| 11/11 [01:02<00:00,  5.71s/it]\n",
      "100%|██████████| 11/11 [01:33<00:00,  8.46s/it]\n",
      "100%|██████████| 11/11 [00:57<00:00,  5.26s/it]\n",
      "100%|██████████| 11/11 [01:15<00:00,  6.82s/it]\n",
      "100%|██████████| 11/11 [01:14<00:00,  6.74s/it]\n",
      "100%|██████████| 11/11 [02:00<00:00, 10.96s/it]\n",
      "100%|██████████| 11/11 [01:18<00:00,  7.10s/it]\n",
      "100%|██████████| 11/11 [01:31<00:00,  8.32s/it]\n",
      "100%|██████████| 11/11 [01:20<00:00,  7.28s/it]\n",
      "100%|██████████| 11/11 [02:05<00:00, 11.44s/it]\n",
      "100%|██████████| 11/11 [01:20<00:00,  7.33s/it]\n",
      "100%|██████████| 11/11 [01:32<00:00,  8.39s/it]\n",
      "100%|██████████| 11/11 [01:19<00:00,  7.26s/it]\n",
      "100%|██████████| 11/11 [02:03<00:00, 11.23s/it]\n",
      "100%|██████████| 11/11 [00:48<00:00,  4.37s/it]\n",
      "100%|██████████| 11/11 [00:49<00:00,  4.54s/it]\n",
      "100%|██████████| 11/11 [00:45<00:00,  4.15s/it]\n",
      "100%|██████████| 11/11 [01:10<00:00,  6.44s/it]\n",
      "100%|██████████| 11/11 [00:44<00:00,  4.01s/it]\n",
      "100%|██████████| 11/11 [00:50<00:00,  4.57s/it]\n",
      "100%|██████████| 11/11 [00:43<00:00,  3.94s/it]\n",
      "100%|██████████| 11/11 [01:08<00:00,  6.20s/it]\n",
      "100%|██████████| 11/11 [00:43<00:00,  4.00s/it]\n",
      "100%|██████████| 11/11 [00:49<00:00,  4.47s/it]\n",
      "100%|██████████| 11/11 [00:45<00:00,  4.13s/it]\n",
      "100%|██████████| 11/11 [01:07<00:00,  6.15s/it]\n",
      "100%|██████████| 11/11 [00:45<00:00,  4.17s/it]\n",
      "100%|██████████| 11/11 [00:49<00:00,  4.51s/it]\n",
      "100%|██████████| 11/11 [00:48<00:00,  4.40s/it]\n",
      "100%|██████████| 11/11 [00:55<00:00,  5.08s/it]\n",
      "100%|██████████| 11/11 [02:36<00:00, 14.22s/it]\n",
      "100%|██████████| 11/11 [03:24<00:00, 18.60s/it]\n",
      "100%|██████████| 11/11 [02:35<00:00, 14.16s/it]\n",
      "100%|██████████| 11/11 [03:53<00:00, 21.27s/it]\n",
      "100%|██████████| 11/11 [02:36<00:00, 14.22s/it]\n",
      "100%|██████████| 11/11 [03:10<00:00, 17.35s/it]\n",
      "100%|██████████| 11/11 [02:38<00:00, 14.37s/it]\n",
      "100%|██████████| 11/11 [04:00<00:00, 21.90s/it]\n",
      "100%|██████████| 11/11 [02:36<00:00, 14.20s/it]\n",
      "100%|██████████| 11/11 [03:12<00:00, 17.51s/it]\n",
      "100%|██████████| 11/11 [02:35<00:00, 14.16s/it]\n",
      "100%|██████████| 11/11 [03:54<00:00, 21.36s/it]\n",
      "100%|██████████| 11/11 [02:34<00:00, 14.05s/it]\n",
      "100%|██████████| 11/11 [03:11<00:00, 17.39s/it]\n",
      "100%|██████████| 11/11 [02:36<00:00, 14.21s/it]\n",
      "100%|██████████| 11/11 [03:51<00:00, 21.06s/it]\n"
     ]
    }
   ],
   "source": [
    "for resize in [100,50,200]:\n",
    "    for dataset_name in [\"new_poisson\",\"subset1\",\"poisson\",\"new_parete\"]:\n",
    "        make_heatmap(dataset_name, resize, gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
