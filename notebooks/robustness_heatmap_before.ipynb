{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# robustness 検証\n",
    "\n",
    "ヒートマップ作成用のデータフレームの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from reserch_utils_HT import network_to_image\n",
    "from models.set_model import CNN_base, D1D2_base\n",
    "from data.data_loader import cnn_data_loader_cv, set_transform\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ba_growth_param(n):\n",
    "    if n >= 50:\n",
    "        params = [1,2,4,6,8,10,15,20,30,40]\n",
    "    elif n == 30:\n",
    "        params = [1,2,4,6,8,10,15,20]\n",
    "    elif n == 20:\n",
    "        params = [1,2,4,6,8,10]\n",
    "    else:\n",
    "        params = [1,2,4,6,8,10,15,20]\n",
    "    return params\n",
    "\n",
    "def attach_params(n):\n",
    "    return [n*.1,n*.25,n*.5,n*.75,n,n*2,n*3,n*4,n*8,n*10,n*15]\n",
    "\n",
    "def pred_there(model, data, class_index, thres):\n",
    "    \"\"\" 閾値を超えて、予測が正解の数を返す \"\"\"\n",
    "    softmax = torch.nn.Softmax(1)\n",
    "    with torch.no_grad():\n",
    "        pred = softmax(model(data))\n",
    "    index = pred.argmax(dim=1) # pred index\n",
    "    count = (pred[index == class_index][:, class_index] > thres).sum().item() # 閾値を超え 且つ 予想が正解 した数\n",
    "    return count\n",
    "\n",
    "def robust_acc_df(model, resize, kind):\n",
    "    kind_to_index = {\"BA\": 0, \"Attach\": 1, \"Growth\": 2, \"Random\": 3}\n",
    "    transform = set_transform(resize)\n",
    "    df = pd.DataFrame()\n",
    "    pred_label_df = pd.DataFrame()\n",
    "    for n in tqdm([20,30,50,70,100,130,200,300,500,1000,2000]):\n",
    "        kind_to_parameters = {\n",
    "            \"BA\": ba_growth_param(n),\n",
    "            \"Growth\": ba_growth_param(n),\n",
    "            \"Attach\": attach_params(n),\n",
    "            \"Random\": [0.01, 0.02, 0.05, 0.07, 0.1, 0.15, 0.2]\n",
    "        }\n",
    "\n",
    "        acc_dict = {} # 精度保存用\n",
    "        for param_index, p in enumerate(kind_to_parameters[kind]):\n",
    "            # network to torch tensor\n",
    "            for i, path in enumerate(glob(f\"./robustness_data_img/{kind}/{n}/{p}/*\")):\n",
    "                img = Image.open(path)\n",
    "                if i == 0:\n",
    "                    data = transform(img).view(1,1,resize,resize)\n",
    "                else:\n",
    "                    img_tensor = transform(img).view(1,1,resize,resize)\n",
    "                    data = torch.cat((data, img_tensor), 0)\n",
    "            \n",
    "            # pred\n",
    "            theres = 0.7 # 閾値設定\n",
    "            acc = pred_there(model, data, kind_to_index[kind], theres) / 50\n",
    "\n",
    "            # save acc\n",
    "            if kind == \"Attach\":\n",
    "                param_name = [\"node*0.1\",\"node*0.25\", \"node*0.5\", \"node*0.75\", \"node\", \"node*2\", \"node*3\", \"node*4\", \"node*8\", \"node*10\", \"node*15\"]\n",
    "                acc_dict.setdefault(param_name[param_index], acc)\n",
    "            else:\n",
    "                acc_dict.setdefault(p, acc)\n",
    "\n",
    "        df = df.append(pd.Series(acc_dict, name=n))\n",
    "    return df.reindex(columns=df.columns[::-1]).T\n",
    "\n",
    "\n",
    "def make_heatmap(dataset_name, resize, gpu=True):\n",
    "    # load model\n",
    "    model = CNN_base(\"CNN\", 4, resize)\n",
    "    model_path = f\"../logs/{dataset_name}/CNN/sort_{resize}_0.001/model_weight/fold0_trial0_epoch10.pth\"\n",
    "    if gpu:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    # make acc dataframe\n",
    "    ba = robust_acc_df(model, resize, \"BA\")\n",
    "    attach = robust_acc_df(model, resize, \"Attach\")\n",
    "    growth = robust_acc_df(model, resize, \"Growth\") \n",
    "    random = robust_acc_df(model, resize, \"Random\")\n",
    "    \n",
    "    ba.to_csv(f\"./robustness_plot/acc_df/CNN_BA_{dataset_name}_{resize}.csv\")\n",
    "    attach.to_csv(f\"./robustness_plot/acc_df/CNN_Attach_{dataset_name}_{resize}.csv\")\n",
    "    growth.to_csv(f\"./robustness_plot/acc_df/CNN_Growth_{dataset_name}_{resize}.csv\")\n",
    "    random.to_csv(f\"./robustness_plot/acc_df/CNN_Random_{dataset_name}_{resize}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:10<00:00,  1.09it/s]\n",
      "100%|██████████| 11/11 [00:11<00:00,  1.01s/it]\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.11it/s]\n",
      "100%|██████████| 11/11 [00:07<00:00,  1.42it/s]\n",
      "100%|██████████| 11/11 [00:18<00:00,  1.72s/it]\n",
      "100%|██████████| 11/11 [00:20<00:00,  1.90s/it]\n",
      "100%|██████████| 11/11 [00:18<00:00,  1.70s/it]\n",
      "100%|██████████| 11/11 [00:14<00:00,  1.30s/it]\n",
      "100%|██████████| 11/11 [00:52<00:00,  4.80s/it]\n",
      "100%|██████████| 11/11 [00:58<00:00,  5.29s/it]\n",
      "100%|██████████| 11/11 [00:51<00:00,  4.68s/it]\n",
      "100%|██████████| 11/11 [00:38<00:00,  3.49s/it]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.07it/s]\n",
      "100%|██████████| 11/11 [00:11<00:00,  1.02s/it]\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.10it/s]\n",
      "100%|██████████| 11/11 [00:07<00:00,  1.39it/s]\n",
      "100%|██████████| 11/11 [00:18<00:00,  1.71s/it]\n",
      "100%|██████████| 11/11 [00:21<00:00,  1.93s/it]\n",
      "100%|██████████| 11/11 [00:18<00:00,  1.71s/it]\n",
      "100%|██████████| 11/11 [00:14<00:00,  1.30s/it]\n",
      "100%|██████████| 11/11 [00:51<00:00,  4.72s/it]\n",
      "100%|██████████| 11/11 [01:00<00:00,  5.47s/it]\n",
      "100%|██████████| 11/11 [00:52<00:00,  4.81s/it]\n",
      "100%|██████████| 11/11 [00:39<00:00,  3.57s/it]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.10it/s]\n",
      "100%|██████████| 11/11 [00:11<00:00,  1.01s/it]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.10it/s]\n",
      "100%|██████████| 11/11 [00:07<00:00,  1.41it/s]\n",
      "100%|██████████| 11/11 [00:19<00:00,  1.73s/it]\n",
      "100%|██████████| 11/11 [00:21<00:00,  1.97s/it]\n",
      "100%|██████████| 11/11 [00:18<00:00,  1.73s/it]\n",
      "100%|██████████| 11/11 [00:14<00:00,  1.32s/it]\n",
      "100%|██████████| 11/11 [00:52<00:00,  4.74s/it]\n",
      "100%|██████████| 11/11 [00:59<00:00,  5.44s/it]\n",
      "100%|██████████| 11/11 [00:51<00:00,  4.72s/it]\n",
      "100%|██████████| 11/11 [00:41<00:00,  3.80s/it]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.04it/s]\n",
      "100%|██████████| 11/11 [00:11<00:00,  1.04s/it]\n",
      "100%|██████████| 11/11 [00:10<00:00,  1.06it/s]\n",
      "100%|██████████| 11/11 [00:08<00:00,  1.36it/s]\n",
      "100%|██████████| 11/11 [00:19<00:00,  1.75s/it]\n",
      "100%|██████████| 11/11 [00:21<00:00,  1.96s/it]\n",
      "100%|██████████| 11/11 [00:19<00:00,  1.74s/it]\n",
      "100%|██████████| 11/11 [00:14<00:00,  1.32s/it]\n",
      "100%|██████████| 11/11 [00:51<00:00,  4.73s/it]\n",
      "100%|██████████| 11/11 [00:59<00:00,  5.43s/it]\n",
      "100%|██████████| 11/11 [00:53<00:00,  4.84s/it]\n",
      "100%|██████████| 11/11 [00:38<00:00,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in [\"subset1\", \"poisson\", \"new_poisson\", \"new_parete\"]:\n",
    "    for resize in [50, 100, 200]:\n",
    "        make_heatmap(dataset_name, resize, gpu=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
