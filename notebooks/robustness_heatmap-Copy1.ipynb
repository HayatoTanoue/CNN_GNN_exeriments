{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# robustness 検証\n",
    "\n",
    "ヒートマップ作成用のデータフレームの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from reserch_utils_HT import network_to_image\n",
    "from models.set_model import CNN_base, D1D2_base\n",
    "from data.data_loader import cnn_data_loader_cv, set_transform\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {\n",
    "    \"BA\": {\n",
    "        20 : [1,2,3,4,5,6,7,8,9,10],\n",
    "        30 : [1,2,3,4,5,6,7,8,9,10],\n",
    "        50 : [1,2,3,4,5,6,7,8,9,10],\n",
    "        70 : [1,2,3,4,5,6,7,8,9,10],\n",
    "        100: [1,2,3,4,5,6,7,8,9,10,15,20,25,30],\n",
    "        200: [1,2,3,4,5,6,7,8,9,10,15,20,25,30],\n",
    "        300: [1,2,3,4,5,6,7,8,9,10,15,20,25,30],\n",
    "        500: [1,2,3,4,5,6,7,8,9,10,15,20,25,30],\n",
    "        700: [1,2,3,4,5,6,7,8,9,10,15,20,25,30],\n",
    "        1000: [1,2,3,4,5,6,7,8,9,10,15,20,25,30,50,100,200],\n",
    "        2000: [1,2,3,4,5,6,7,8,9,10,15,20,25,30,50,100,200]\n",
    "    },\n",
    "    \"Growth\": {\n",
    "        20 : [1,2,3,4,5,6,7,8,9,10],\n",
    "        30 : [1,2,3,4,5,6,7,8,9,10],\n",
    "        50 : [1,2,3,4,5,6,7,8,9,10],\n",
    "        70 : [1,2,3,4,5,6,7,8,9,10],\n",
    "        100: [1,2,3,4,5,6,7,8,9,10,15,20,25,30],\n",
    "        200: [1,2,3,4,5,6,7,8,9,10,15,20,25,30],\n",
    "        300: [1,2,3,4,5,6,7,8,9,10,15,20,25,30],\n",
    "        500: [1,2,3,4,5,6,7,8,9,10,15,20,25,30],\n",
    "        700: [1,2,3,4,5,6,7,8,9,10,15,20,25,30],\n",
    "        1000: [1,2,3,4,5,6,7,8,9,10,15,20,25,30,50,100,200],\n",
    "        2000: [1,2,3,4,5,6,7,8,9,10,15,20,25,30,50,100,200]\n",
    "    },\n",
    "    \"Attach\":{\n",
    "        20 : [20 * i for i in np.linspace(0.5, 25, 10)],\n",
    "        30 : [30 * i for i in np.linspace(0.5, 25, 10)],\n",
    "        50 : [50 * i for i in np.linspace(0.5, 25, 10)],\n",
    "        70 : [70 * i for i in np.linspace(0.5, 25, 10)],\n",
    "        \n",
    "        100 : [100 * i for i in np.linspace(0.5, 25, 14)],\n",
    "        200 : [200 * i for i in np.linspace(0.5, 25, 14)],\n",
    "        300 : [300 * i for i in np.linspace(0.5, 25, 14)],\n",
    "        500 : [500 * i for i in np.linspace(0.5, 25, 14)],\n",
    "        700 : [700 * i for i in np.linspace(0.5, 25, 14)],\n",
    "        1000 : [1000 * i for i in np.linspace(0.5, 25, 17)],\n",
    "        2000 : [2000 * i for i in np.linspace(0.5, 25, 17)],\n",
    "        \n",
    "    },\n",
    "    \"Random\": {\n",
    "        20: np.logspace(-1.3, -0.1, 10),\n",
    "        30: np.logspace(-1.3, -0.1, 10),\n",
    "        50: np.logspace(-1.3, -0.1, 10),\n",
    "        70: np.logspace(-1.3, -0.1, 10),\n",
    "        100: np.logspace(-2, -0.7, 14),\n",
    "        200: np.logspace(-2, -0.7, 14),\n",
    "        300: np.logspace(-2, -0.7, 14),\n",
    "        500: np.logspace(-2, -0.7, 14),\n",
    "        700: np.logspace(-2, -0.7, 14),\n",
    "        1000: np.logspace(-3., -0.9, 17),\n",
    "        2000: np.logspace(-3., -0.9, 17)\n",
    "    }\n",
    "}\n",
    "\n",
    "nodes = [20,30,50,70,100,200,300,500,700,1000,2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_to_tensor(resize, kind, n, p):\n",
    "    \"\"\" データの読み込み, tensor結合 \"\"\"\n",
    "    transform = set_transform(resize)\n",
    "    if kind == \"Attach\":\n",
    "        p = int(p)\n",
    "    for i in range(100):\n",
    "        img = Image.open(f\"./robustness_data_img/{kind}/{n}/{p}/{i}.png\")\n",
    "        if i == 0:\n",
    "            data = transform(img).view(1,1,resize,resize)\n",
    "        else:\n",
    "            img_tensor = transform(img).view(1,1,resize,resize)\n",
    "            data = torch.cat((data, img_tensor), 0)\n",
    "    return data\n",
    "\n",
    "def make_pred_df(model, data, kind, n, p, cnt):\n",
    "    \"\"\" ネットワークごとの予測ラベルと尤度データ作成 \"\"\"\n",
    "    kind_to_label = {\"BA\": 0, \"Attach\": 1, \"Growth\": 2, \"Random\": 3}\n",
    "    softmax = torch.nn.Softmax(1)\n",
    "    with torch.no_grad():\n",
    "        pred = softmax(model(data))\n",
    "    index = pred.argmax(dim=1) # pred index\n",
    "\n",
    "    pred_df = pd.DataFrame()\n",
    "    pred_df[\"seed\"] = np.array(range(100)) + 10000 # seed\n",
    "    pred_df[\"node\"] = n\n",
    "    pred_df[\"parameter\"] = p\n",
    "    pred_df[\"kind\"] = kind\n",
    "    pred_df[\"true_label\"] = kind_to_label[kind]\n",
    "    pred_df[\"pred\"] = index\n",
    "    pred_df[\"probablility\"] = [pred[i, idx].item() for i, idx in enumerate(index)]\n",
    "    \n",
    "    return pred_df\n",
    "\n",
    "def robust_acc_df(model, resize, kind, parameter):\n",
    "    kind_to_index = {\"BA\": 0, \"Attach\": 1, \"Growth\": 2, \"Random\": 3}\n",
    "    transform = set_transform(resize)\n",
    "    df = pd.DataFrame()\n",
    "    cnt = 0\n",
    "    for n in tqdm([20,30,50,70,100,200,300,500,700,1000,2000]):\n",
    "        params = parameter[kind][n]\n",
    "        for param_index, p in enumerate(params):\n",
    "            # image to torch tensor\n",
    "            data = load_image_to_tensor(resize, kind, n, p)\n",
    "            pred_df = make_pred_df(model, data, kind, n, p, cnt)\n",
    "            cnt += 100\n",
    "            df = df.append(pred_df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_heatmap(dataset_name, resize, parameter, gpu=True):\n",
    "    # load model\n",
    "    model = CNN_base(\"CNN\", 4, resize)\n",
    "    model_path = f\"../logs/{dataset_name}/CNN/sort_{resize}_0.001/model_weight/fold0_trial0_epoch10.pth\"\n",
    "    if gpu:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    # make acc dataframe\n",
    "    pred_df = pd.DataFrame()\n",
    "    for kind in [\"BA\", \"Attach\", \"Growth\", \"Random\"]:\n",
    "        pred_df = pred_df.append(robust_acc_df(model, resize, kind, parameter))\n",
    "    \n",
    "    pred_df.to_csv(f\"./robustness_plot/CNN_{dataset_name}_{resize}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:33<00:00,  3.09s/it]\n",
      "100%|██████████| 11/11 [00:36<00:00,  3.30s/it]\n",
      "100%|██████████| 11/11 [00:36<00:00,  3.30s/it]\n",
      "100%|██████████| 11/11 [00:33<00:00,  3.06s/it]\n",
      "100%|██████████| 11/11 [00:58<00:00,  5.35s/it]\n",
      "100%|██████████| 11/11 [00:57<00:00,  5.22s/it]\n",
      "100%|██████████| 11/11 [00:58<00:00,  5.29s/it]\n",
      "100%|██████████| 11/11 [00:58<00:00,  5.28s/it]\n",
      "100%|██████████| 11/11 [02:35<00:00, 14.13s/it]\n",
      "100%|██████████| 11/11 [02:31<00:00, 13.81s/it]\n",
      "100%|██████████| 11/11 [02:32<00:00, 13.90s/it]\n",
      "100%|██████████| 11/11 [02:34<00:00, 14.03s/it]\n",
      "100%|██████████| 11/11 [00:35<00:00,  3.23s/it]\n",
      "100%|██████████| 11/11 [00:33<00:00,  3.08s/it]\n",
      "100%|██████████| 11/11 [00:34<00:00,  3.13s/it]\n",
      "100%|██████████| 11/11 [00:34<00:00,  3.15s/it]\n",
      "100%|██████████| 11/11 [01:00<00:00,  5.51s/it]\n",
      "100%|██████████| 11/11 [00:59<00:00,  5.38s/it]\n",
      "100%|██████████| 11/11 [01:00<00:00,  5.48s/it]\n",
      "100%|██████████| 11/11 [01:00<00:00,  5.52s/it]\n",
      "100%|██████████| 11/11 [02:34<00:00, 14.06s/it]\n",
      "100%|██████████| 11/11 [02:32<00:00, 13.89s/it]\n",
      "100%|██████████| 11/11 [02:32<00:00, 13.84s/it]\n",
      "100%|██████████| 11/11 [02:32<00:00, 13.85s/it]\n",
      "100%|██████████| 11/11 [00:34<00:00,  3.12s/it]\n",
      "100%|██████████| 11/11 [00:33<00:00,  3.05s/it]\n",
      "100%|██████████| 11/11 [00:34<00:00,  3.13s/it]\n",
      "100%|██████████| 11/11 [00:34<00:00,  3.12s/it]\n",
      "100%|██████████| 11/11 [00:58<00:00,  5.34s/it]\n",
      "100%|██████████| 11/11 [00:57<00:00,  5.27s/it]\n",
      "100%|██████████| 11/11 [00:58<00:00,  5.35s/it]\n",
      "100%|██████████| 11/11 [00:59<00:00,  5.37s/it]\n",
      "100%|██████████| 11/11 [02:28<00:00, 13.53s/it]\n",
      "100%|██████████| 11/11 [02:28<00:00, 13.52s/it]\n",
      "100%|██████████| 11/11 [02:30<00:00, 13.70s/it]\n",
      "100%|██████████| 11/11 [02:29<00:00, 13.59s/it]\n",
      "100%|██████████| 11/11 [00:34<00:00,  3.12s/it]\n",
      "100%|██████████| 11/11 [00:33<00:00,  3.02s/it]\n",
      "100%|██████████| 11/11 [00:34<00:00,  3.13s/it]\n",
      "100%|██████████| 11/11 [00:34<00:00,  3.12s/it]\n",
      "100%|██████████| 11/11 [00:58<00:00,  5.32s/it]\n",
      "100%|██████████| 11/11 [00:57<00:00,  5.25s/it]\n",
      "100%|██████████| 11/11 [00:58<00:00,  5.33s/it]\n",
      "100%|██████████| 11/11 [00:58<00:00,  5.34s/it]\n",
      "100%|██████████| 11/11 [02:30<00:00, 13.67s/it]\n",
      "100%|██████████| 11/11 [02:28<00:00, 13.51s/it]\n",
      "100%|██████████| 11/11 [02:29<00:00, 13.55s/it]\n",
      "100%|██████████| 11/11 [02:32<00:00, 13.89s/it]\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in [\"subset1\", \"poisson\", \"new_poisson\", \"new_parete\"]:\n",
    "    for resize in [50, 100, 200]:\n",
    "        make_heatmap(dataset_name, resize, parameter, gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
